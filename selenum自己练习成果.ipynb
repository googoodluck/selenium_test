{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "212b1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import platform\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "我利用driver爬取了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import platform\n",
    "\n",
    "# 启动Chrome浏览器\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 打开网页\n",
    "website_URL = \"https://www.fst.um.edu.mo/people/academic-staff/\"\n",
    "\n",
    "\n",
    "driver.get(website_URL)\n",
    "\n",
    "# 定位所有教职工信息的父元素\n",
    "# staff_elements = driver.find_elements(By.XPATH, \"//td[@class='acadstaff_name']\")\n",
    "# 循环处理每个教职工信息\n",
    "# for staff_element in staff_elements:\n",
    "#     name = staff_element.text\n",
    "#     phone = staff_element.find_element(By.XPATH, \"following-sibling::td[1]\").text\n",
    "#     position = staff_element.find_element(By.XPATH, \"following-sibling::td[2]\").text\n",
    "#     office = staff_element.find_element(By.XPATH, \"following-sibling::td[3]\").text\n",
    "#     department = staff_element.find_element(By.XPATH, \"following-sibling::td[4]\").text\n",
    "    \n",
    "#     print(\"Name:\", name)\n",
    "#     print(\"Phone:\", phone)\n",
    "#     print(\"Position:\", position)\n",
    "#     print(\"Office:\", office)\n",
    "#     print(\"Department:\", department)\n",
    "#     print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bde787",
   "metadata": {},
   "outputs": [],
   "source": [
    "爬取单个主页的教师信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20b8dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information: Yonghua SONG宋永華\n",
      "Chair Professor\n",
      "Rector\n",
      "Director of State Key Laboratory of Internet of Things for Smart City\n",
      "DSc FRSA FIEEE FIET\n",
      "Fellow of Royal Academy of Engineering of the United Kingdom\n",
      "Member of the Academia Europaea\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "website_URL =\"https://www.fst.um.edu.mo/people/yhsong/\"\n",
    "driver.get(website_URL) \n",
    "time.sleep(2)\n",
    "parent_element = driver.find_element(By.XPATH,\"//td[@class='staffphoto']/..\")\n",
    "photo_element = parent_element.find_element(By.XPATH,\"//td[@class='staffphoto']\")\n",
    "# info_element = parent_element.find_element(By.XPATH,\"//td[@class='staffphoto']/following-sibling::td\")\n",
    "info_element = parent_element.find_element(By.XPATH,\"//*[@id='post-2708']/div/div/div/div/div/div[1]/table/tbody/tr/td[2]\") #同上一行作用\n",
    "\n",
    "\n",
    "# # photo_content = photo_element.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "# 相同作用都是提取src\n",
    "# photo_content = photo_element.find_element(By.XPATH,\"//*[@id='post-2708']/div/div/div/div/div/div[1]/table/tbody/tr/td[1]/img\").get_attribute(\"src\")\n",
    "\n",
    "\n",
    "info_content = info_element.text\n",
    "\n",
    "print(\"Photo URL:\", photo_content)\n",
    "print(\"Information:\", info_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "两者结合,循环获取每个老师主页内的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e709f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Yonghua SONG (宋永華)\n",
      "Link: https://www.fst.um.edu.mo/people/yhsong/\n",
      "Photo URL: https://www.fst.um.edu.mo/image/staff-photo/yhsong.jpg\n",
      "Information: Yonghua SONG宋永華\n",
      "Chair Professor\n",
      "Rector\n",
      "Director of State Key Laboratory of Internet of Things for Smart City\n",
      "DSc FRSA FIEEE FIET\n",
      "Fellow of Royal Academy of Engineering of the United Kingdom\n",
      "Member of the Academia Europaea\n",
      "Name: Rui Paulo da Silva MARTINS (馬許願)\n",
      "Link: https://www.fst.um.edu.mo/people/rtorpm/\n",
      "Photo URL: https://www.fst.um.edu.mo/image/staff-photo/rtorpm.jpg\n",
      "Information: Rui Paulo da Silva MARTINS 馬許願\n",
      "Chair Professor\n",
      "Vice-Rector (Global Affairs)\n",
      "Director of the Institute of Microelectronics\n",
      "Fellow of IEEE (USA)\n",
      "Full Member of the Academy of Sciences of Lisbon, Portugal\n",
      "Name: Cheng-Zhong XU (須成忠)\n",
      "Link: https://www.fst.um.edu.mo/people/czxu/\n"
     ]
    }
   ],
   "source": [
    "### from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# 打开网页\n",
    "website_URL = \"https://www.fst.um.edu.mo/people/academic-staff/\"\n",
    "\n",
    "# 设置Chrome浏览器无头模式\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 启用无头模式\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # 禁用GPU加速\n",
    "\n",
    "# 创建Chrome浏览器实例\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(website_URL)\n",
    "\n",
    "# 获取初始窗口句柄\n",
    "initial_window = driver.current_window_handle\n",
    "\n",
    "# 找到所有包含名字链接的元素\n",
    "name_elements = driver.find_elements(By.XPATH, \"//td[@class='acadstaff_name']/a\")\n",
    "\n",
    "# 循环点击每个名字链接\n",
    "for name_element in name_elements:\n",
    "    name = name_element.text\n",
    "    link = name_element.get_attribute(\"href\")\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Link:\", link)\n",
    "    \n",
    "    # 点击链接\n",
    "    name_element.click()\n",
    "    #加载新页面信息\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        photo_element = wait.until(EC.visibility_of_element_located((By.XPATH, \"//td[@class='staffphoto']\")))\n",
    "        info_element = wait.until(EC.visibility_of_element_located((By.XPATH, \"//td[@class='staffphoto']/following-sibling::td\")))\n",
    "\n",
    "        photo_content = photo_element.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "        info_content = info_element.text\n",
    "\n",
    "        print(\"Photo URL:\", photo_content)\n",
    "        print(\"Information:\", info_content)\n",
    "    except TimeoutException as e:\n",
    "        print(\"TimeoutException occurred. Special case encountered.\")\n",
    "        # 处理特殊情况\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        # 处理其他异常情况\n",
    "\n",
    "\n",
    "    \n",
    "    # 在这里可以添加进一步操作，比如获取页面信息或其他操作\n",
    "    \n",
    "    # 检查当前窗口是否是新标签页\n",
    "    if len(driver.window_handles) > 1:\n",
    "        # 获取新标签页的窗口句柄\n",
    "        new_window = [window for window in driver.window_handles if window != initial_window][0]\n",
    "        \n",
    "        # 切换到新标签页\n",
    "        driver.switch_to.window(new_window)\n",
    "        \n",
    "        # 处理新标签页的操作，例如获取信息、执行操作等\n",
    "        \n",
    "        # 关闭新标签页\n",
    "        driver.close()\n",
    "        \n",
    "        # 切换回初始页面\n",
    "        driver.switch_to.window(initial_window)\n",
    "    else:\n",
    "        driver.back()\n",
    "\n",
    "# 关闭浏览器\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "通过driver获取书籍对应的link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d255c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n"
     ]
    }
   ],
   "source": [
    "# 打开网页\n",
    "book_url = \"https://books.toscrape.com/\"\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(book_url)\n",
    "link  = driver.find_element(By.XPATH,'//*[@id=\"default\"]/div[1]/div/div/div/section/div[2]/ol/li[1]/article/h3/a').get_attribute('href')\n",
    "#等价于\n",
    "# element = driver.find_element(By.XPATH,\"//article[@class='product_pod']/div/a\").get_attribute('href')\n",
    "# soup.find(\"article\", class_ = \"product_pod\").div.a.get('href')\n",
    "\n",
    "print(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d222dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fetched products URLs\n",
      "One example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_page_products_urls = [link.get_attribute('href') for link in driver.find_elements(By.XPATH,'//*[@id=\"default\"]/div[1]/div/div/div/section/div[2]/ol/li[1]/article/h3/a')]\n",
    "\n",
    "#等价于\n",
    "# main_page_products_urls = [x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "\n",
    "print(str(len(main_page_products_urls)) + \" fetched products URLs\")\n",
    "print(\"One example:\")\n",
    "main_page_products_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1218b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "获取书籍不同分类的的link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05038a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/category/books_1/index.html\n",
      "https://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html\n",
      "https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html\n"
     ]
    }
   ],
   "source": [
    "book_url = \"https://books.toscrape.com/\"\n",
    "result = \"/\".join(url.split(\"/\")[:-1])\n",
    "\n",
    "book_category= driver.find_elements(By.XPATH,\"//div[@class='side_categories']//a\")\n",
    "\n",
    "# print(book_category)\n",
    "for e in book_category[:5]:\n",
    "    print(e.get_attribute('href'))\n",
    "# 等价于\n",
    "# categories_urls = [main_url + x.get('href') for x in soup.find_all(\"a\", href=re.compile(\"catalogue/category/books\"))]\n",
    "# categories_urls = categories_urls[1:] # we remove the first one because it corresponds to all the books\n",
    "\n",
    "# print(str(len(categories_urls)) + \" fetched categories URLs\")\n",
    "# print(\"Some examples:\")\n",
    "# categories_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "获取所有的的书籍商店页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cac144f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com/catalogue/page-1.html\n",
      "http://books.toscrape.com/catalogue/page-2.html\n",
      "http://books.toscrape.com/catalogue/page-3.html\n",
      "http://books.toscrape.com/catalogue/page-4.html\n",
      "http://books.toscrape.com/catalogue/page-5.html\n",
      "http://books.toscrape.com/catalogue/page-6.html\n",
      "http://books.toscrape.com/catalogue/page-7.html\n",
      "http://books.toscrape.com/catalogue/page-8.html\n",
      "http://books.toscrape.com/catalogue/page-9.html\n",
      "http://books.toscrape.com/catalogue/page-10.html\n",
      "http://books.toscrape.com/catalogue/page-11.html\n",
      "http://books.toscrape.com/catalogue/page-12.html\n",
      "http://books.toscrape.com/catalogue/page-13.html\n",
      "http://books.toscrape.com/catalogue/page-14.html\n",
      "http://books.toscrape.com/catalogue/page-15.html\n",
      "http://books.toscrape.com/catalogue/page-16.html\n",
      "http://books.toscrape.com/catalogue/page-17.html\n",
      "http://books.toscrape.com/catalogue/page-18.html\n",
      "http://books.toscrape.com/catalogue/page-19.html\n",
      "http://books.toscrape.com/catalogue/page-20.html\n",
      "http://books.toscrape.com/catalogue/page-21.html\n",
      "http://books.toscrape.com/catalogue/page-22.html\n",
      "http://books.toscrape.com/catalogue/page-23.html\n",
      "http://books.toscrape.com/catalogue/page-24.html\n",
      "http://books.toscrape.com/catalogue/page-25.html\n",
      "http://books.toscrape.com/catalogue/page-26.html\n",
      "http://books.toscrape.com/catalogue/page-27.html\n",
      "http://books.toscrape.com/catalogue/page-28.html\n",
      "http://books.toscrape.com/catalogue/page-29.html\n",
      "http://books.toscrape.com/catalogue/page-30.html\n",
      "http://books.toscrape.com/catalogue/page-31.html\n",
      "http://books.toscrape.com/catalogue/page-32.html\n",
      "http://books.toscrape.com/catalogue/page-33.html\n",
      "http://books.toscrape.com/catalogue/page-34.html\n",
      "http://books.toscrape.com/catalogue/page-35.html\n",
      "http://books.toscrape.com/catalogue/page-36.html\n",
      "http://books.toscrape.com/catalogue/page-37.html\n",
      "http://books.toscrape.com/catalogue/page-38.html\n",
      "http://books.toscrape.com/catalogue/page-39.html\n",
      "http://books.toscrape.com/catalogue/page-40.html\n",
      "http://books.toscrape.com/catalogue/page-41.html\n",
      "http://books.toscrape.com/catalogue/page-42.html\n",
      "http://books.toscrape.com/catalogue/page-43.html\n",
      "http://books.toscrape.com/catalogue/page-44.html\n",
      "http://books.toscrape.com/catalogue/page-45.html\n",
      "http://books.toscrape.com/catalogue/page-46.html\n",
      "http://books.toscrape.com/catalogue/page-47.html\n",
      "http://books.toscrape.com/catalogue/page-48.html\n",
      "http://books.toscrape.com/catalogue/page-49.html\n",
      "http://books.toscrape.com/catalogue/page-50.html\n",
      "No next link found. Exiting loop.\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 打开网页\n",
    "# book_url = [\"https://books.toscrape.com/\"]\n",
    "# driver = webdriver.Chrome()\n",
    "\n",
    "# 设置Chrome浏览器无头模式\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # 启用无头模式\n",
    "# chrome_options.add_argument(\"--disable-gpu\")  # 禁用GPU加速\n",
    "# url=book_url[0]\n",
    "\n",
    "\n",
    "# 获取link\n",
    "# next_link = driver.find_element(By.XPATH, \"//a[text()='next']\").get_attribute('href')\n",
    "# next_link = driver.find_element(By.XPATH, \"//li[@class='next']/a\").get_attribute('href')\n",
    "\n",
    "\n",
    "# 循环获取新的href并跳转页面\n",
    "# while True:\n",
    "#     # 打开当前页面\n",
    "#     driver.get(url)\n",
    "\n",
    "#     try:\n",
    "#         # 获取当前页面的下一页链接\n",
    "#         next_link = driver.find_element(By.XPATH, \"//li[@class='next']/a\").get_attribute('href')\n",
    "        \n",
    "#         # 输出当前页面的链接\n",
    "#         book_url.append(next_link)\n",
    "\n",
    "#         # 更新url为下一页链接\n",
    "#         url = next_link\n",
    "\n",
    "#     except:\n",
    "#         # 如果无法找到下一页链接，跳出循环\n",
    "#         print(\"No next link found. Exiting loop.\")\n",
    "#         break\n",
    "\n",
    "# # 关闭浏览器\n",
    "# driver.quit()\n",
    "# print(book_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# store all the results into a list\n",
    "# pages_urls = [main_url]\n",
    "# soup = getAndParseURL(pages_urls[0])\n",
    "\n",
    "# 这个运行得更快\n",
    "# # while we get two matches, this means that the webpage contains a 'previous' and a 'next' button\n",
    "# # if there is only one button, this means that we are either on the first page or on the last page\n",
    "# # we stop when we get to the last page\n",
    "\n",
    "# while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(pages_urls) == 1:\n",
    "    \n",
    "#     # get the new complete url by adding the fetched URL to the base URL (and removing the .html part of the base URL)\n",
    "#     new_url = \"/\".join(pages_urls[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "    \n",
    "#     # add the URL to the list\n",
    "#     pages_urls.append(new_url)\n",
    "    \n",
    "#     # parse the next page\n",
    "#     soup = getAndParseURL(new_url)\n",
    "    \n",
    "\n",
    "# print(str(len(pages_urls)) + \" fetched URLs\")\n",
    "# print(\"Some examples:\")\n",
    "# pages_urls[:5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#这个还要更简单\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "pages_urls = []\n",
    "\n",
    "new_page = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "while requests.get(new_page).status_code == 200:\n",
    "    response = requests.get(new_page)  # 将URL替换为您要访问的网页URL\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# 定位下一页链接\n",
    "    pages_urls.append(new_page)\n",
    "    print(new_page)\n",
    "    try:\n",
    "        next_link_element = soup.find('li', class_='next').find('a')\n",
    "        new_page = pages_urls[-1].split(\"-\")[0] + \"-\" +next_link_element['href'].split(\"-\")[1]\n",
    "\n",
    "    except:\n",
    "        # 如果无法找到下一页链接，跳出循环\n",
    "        print(\"No next link found. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "print(len(pages_urls))\n",
    "\n",
    "#以下感觉不是很完美,页面不一定是每次都+1,因此用的上面那套\n",
    "# new_page = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "# while requests.get(new_page).status_code == 200:\n",
    "#     pages_urls.append(new_page)\n",
    "#     new_page = pages_urls[-1].split(\"-\")[0] + \"-\" + str(int(pages_urls[-1].split(\"-\")[1].split(\".\")[0]) + 1) + \".html\"\n",
    "#     print(pages_urls[-1].split(\"-\")[0],str(int(pages_urls[-1].split(\"-\")[1].split(\".\")[0]) + 1))\n",
    "# print(str(len(pages_urls)) + \" fetched URLs\")\n",
    "# print(\"Some examples:\")\n",
    "# pages_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "获取每一页上,每一本书籍的链接,共一千个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f721b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mdiv\u001b[38;5;241m.\u001b[39ma\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_pod\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m     14\u001b[0m booksURLs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages_urls:\n\u001b[0;32m     16\u001b[0m     booksURLs\u001b[38;5;241m.\u001b[39mextend(getBooksURLs(page))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(booksURLs)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m fetched URLs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages_urls' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def getAndParseURL(url):\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def getBooksURLs(url):\n",
    "    soup = getAndParseURL(url)\n",
    "    # remove the index.html part of the base url before returning the results\n",
    "    return([\"/\".join(url.split(\"/\")[:-1]) + \"/\" + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")])\n",
    "\n",
    "\n",
    "booksURLs = []\n",
    "for page in pages_urls:\n",
    "    booksURLs.extend(getBooksURLs(page))\n",
    "    \n",
    "print(str(len(booksURLs)) + \" fetched URLs\")\n",
    "print(\"Some examples:\")\n",
    "booksURLs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "获取每一本的详细信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bfd735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html']\n",
      "0    A Light in the Attic\n",
      "Name: name, dtype: object\n",
      "\n",
      "0    51.77\n",
      "Name: price, dtype: object\n",
      "\n",
      "0    22\n",
      "Name: nb_in_stock, dtype: object\n",
      "\n",
      "0    https://books.toscrape.com/catalogue/a-light-i...\n",
      "Name: url_img, dtype: object\n",
      "\n",
      "0    poetry_23\n",
      "Name: product_category, dtype: object\n",
      "\n",
      "0    Three\n",
      "Name: rating, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "names = []\n",
    "prices = []\n",
    "nb_in_stock = []\n",
    "img_urls = []\n",
    "categories = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "\n",
    "text =['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html']\n",
    "print(text)\n",
    "# scrape data for every book URL: this may take some time\n",
    "for url in text:\n",
    "    soup = getAndParseURL(url)\n",
    "    # product name\n",
    "    names.append(soup.find(\"div\", class_=re.compile(\"product_main\")).h1.text)\n",
    "    # product price\n",
    "    prices.append(soup.find(\"p\", class_=\"price_color\").text[2:])  # get rid of the pound sign\n",
    "    # number of available products\n",
    "    nb_in_stock.append(\n",
    "        re.sub(\"[^0-9]\", \"\", soup.find(\"p\", class_=\"instock availability\").text))  # get rid of non numerical characters\n",
    "    # image url\n",
    "    img_urls.append(url.replace(\"index.html\", \"\") + soup.find(\"img\").get(\"src\"))\n",
    "    # product category\n",
    "    categories.append(soup.find(\"a\", href=re.compile(\"../category/books/\")).get(\"href\").split(\"/\")[3])\n",
    "    # ratings\n",
    "    ratings.append(soup.find(\"p\", class_=re.compile(\"star-rating\")).get(\"class\")[1])\n",
    "\n",
    "# add data into pandas df\n",
    "import pandas as pd\n",
    "\n",
    "scraped_data = pd.DataFrame(\n",
    "    {'name': names, 'price': prices, 'nb_in_stock': nb_in_stock, \"url_img\": img_urls, \"product_category\": categories,\n",
    "     \"rating\": ratings})\n",
    "for col in scraped_data.columns:\n",
    "    print(scraped_data[col].head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "通过request模拟澳大老师信息的爬取"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
